# AutoRia Scraper

A containerized asynchronous web scraper for collecting car listing details from [auto.ria.com](https://auto.ria.com) using **Playwright**, **aiohttp**, and **PostgreSQL**. Designed to run periodically via **cron**.

---

## ğŸ“ Project Structure

```
autoria-scraper/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py              # Main entry point to launch the scraper
â”‚   â”œâ”€â”€ db.py                # Async database logic (check/insert)
â”‚   â”œâ”€â”€ utils.py             # HTML helpers (e.g., extract secure hash)
â”‚   â””â”€â”€ settings.py          # Config constants (BASE_URL, DB URL, etc.)
â”œâ”€â”€ db_init/
â”‚   â””â”€â”€ init.sql             # SQL script to initialize 'cars' table
â”œâ”€â”€ dumps/                   # Directory where database dumps are stored
â”œâ”€â”€ .env.template            # Template environment config
â”œâ”€â”€ Dockerfile               # Docker build for Playwright + app
â”œâ”€â”€ docker-compose.yml       # Multi-container orchestration
â”œâ”€â”€ crontab                  # Daily & test (every minute) schedules           
â””â”€â”€ README.md                # You're reading this file
```

---

## âš™ï¸ Requirements

- [Docker](https://www.docker.com/)
- [Docker Compose](https://docs.docker.com/compose/)
- `.env` file with correct configuration (see below)

---

## ğŸ§ª Environment Variables

Duplicate `.env.template` into `.env` and fill in your own config if needed:

```env
POSTGRES_USER=auto_ria_user
POSTGRES_PASSWORD=auto_ria_password
POSTGRES_DB=auto_ria_db
DATABASE_URL=postgresql://auto_ria_user:auto_ria_password@db:5432/auto_ria_db
```

---

## ğŸš€ Setup & Run

### 1. Build & Start Containers

```bash
docker-compose up --build -d
```

This will:

- Launch PostgreSQL database
- Initialize schema from `init.sql`
- Start cron which executes scraper until container is stopped

---

## ğŸ•’ Scheduling via Cron

Two presets:

- `* * * * *` â€“ every minute (for testing)
- `0 0 * * *` â€“ every day at midnight

Modify `crontab` as needed and ensure the container runs with cron enabled (or use host cron to invoke it).

---

## ğŸ§¼ Database Dump

After every successful scraping run, a dump is created under `/dumps/` folder inside container (mounted to host). Dumps are in SQL format:

```
dumps/dump_YYYY-MM-DD_HH-MM-SS.sql
```

---

## ğŸ§© Features

- âœ… Asynchronous scraping with `aiohttp` + `playwright`
- âœ… Phone number resolution via API
- âœ… Graceful handling of deleted listings
- âœ… Deduplication (skip already parsed cars)
- âœ… PostgreSQL auto-init with schema
- âœ… Dumps of DB contents
- âœ… Easy to run with Docker & Docker Compose

---

## ğŸ§± Table Schema

```sql
CREATE TABLE IF NOT EXISTS cars (
    id SERIAL PRIMARY KEY,
    url VARCHAR(255) NOT NULL UNIQUE,
    title VARCHAR(255) NOT NULL,
    price_usd INTEGER NOT NULL,
    odometer INTEGER NOT NULL,
    username VARCHAR(255) NOT NULL,
    phone_number BIGINT NOT NULL,
    image_url VARCHAR(255) NOT NULL,
    images_count INTEGER NOT NULL,
    car_number VARCHAR(255) NOT NULL,
    car_vin VARCHAR(255) NOT NULL,
    datetime_found TIMESTAMP WITHOUT TIME ZONE DEFAULT NOW()
);
```

---

## ğŸ” Notes

- The scraper uses `Playwright` to simulate clicks on â€œShow phoneâ€ and extract data from scripts.
- Rate limits are respected via randomized `sleep()` and retry logic.
- You must use the correct version of `pg_dump` matching PostgreSQL server. Optionally add `postgres-client` to the scraper Dockerfile.

---

## ğŸ”§ Ways to Improve

- **Implement batching (DB & requests):**  
  Insert data in batches instead of per-record, and send multiple requests in parallel for higher throughput and lower DB overhead.
- **Retry with exponential backoff:**  
  Use smarter retry logic for requests and API calls, especially on rate-limits or temporary failures.
- **Async connection pool:**  
  Consider using a connection pool for aiohttp to further improve efficiency.
- **Proxy and user-agent rotation:**  
  Reduce risk of blocking or bans by rotating proxies and user-agents for each session/request.

---

## ğŸ“œ License

MIT â€“ do whatever you want, but don't DDoS auto.ria ğŸ™‚
